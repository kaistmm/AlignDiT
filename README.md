# AlignDiT

Official PyTorch implementation for the following paper:
> **AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation**<br>
> [Jeongsoo Choi](https://choijeongsoo.github.io), [Ji-Hoon Kim](https://sites.google.com/view/jhoonkim), [Kim Sung-Bin](https://sites.google.com/view/kimsungbin), [Tae-Hyun Oh](https://ami.kaist.ac.kr/members/tae-hyun-oh), [Joon Son Chung](https://mm.kaist.ac.kr/joon)<br>
> ACM MM 2025<br>
> \[[Paper](https://arxiv.org/abs/2504.20629)\] \[[Project](https://mm.kaist.ac.kr/projects/AlignDiT)\]

<div align="center"><img width="80%" src="https://mm.kaist.ac.kr/projects/AlignDiT/imgs/fig1.png?raw=true"/></div>

## Datasets
To help verify your setup and ensure reproducibility, we provide the following debug data.

  Path | Dataset | Debug data
  :---:|:---:|:---:
  data/LibriSpeech_debug | [LibriSpeech](https://www.openslr.org/12) | [download](https://drive.google.com/file/d/1tO8qcddtfhL7yh_svDr9Jaif-mNHdflL/view)
  data/LRS3_debug | [LRS3](https://mmai.io/datasets/lip_reading) | [download](https://drive.google.com/file/d/1u-AbgT_O4Xl14XKnui-IVCMoA0mZEJRU/view)

## Model Checkpoints
  Path | Train Dataset | Model
  :---:|:---:|:---:
  ckpts/AlignDiT_pretrain_hifigan_16k_LibriSpeech_notext/model_500000.pt | [LibriSpeech](https://www.openslr.org/12) | [download](https://drive.google.com/file/d/1-LddbcvUQmPOy_iLJ7I85eaMaYx1kgav/view)
  ckpts/AlignDiT_finetune_hifigan_16k_LRS3_char/model_400000.pt | [LRS3](https://mmai.io/datasets/lip_reading) | [download](https://drive.google.com/file/d/1gDRpB-5UKYy3NDz6uysNNMZQ25leXLrV/view)

## Test Samples
We provide audio samples generated by AlignDiT. For VTS task, we use a lip reading model ([Auto-AVSR](https://github.com/mpc001/auto_avsr)) to transcribe text from the silent video before inference.

  Task | Test Dataset | WER ↓ | AVSync ↑ | spkSIM ↑ | Samples
  :---:|:---:|:---:|:---:|:---:|:---:|
  ADR (automated dialogue replacement) | [LRS3](https://mmai.io/datasets/lip_reading) | 1.401 | 0.751 | 0.515 | [download](https://drive.google.com/file/d/1amU29Ax_E2USyJz5IjXwC9GFaBGlnCDk/view)
  VTS (video-to-speech synthesis) | [LRS3](https://mmai.io/datasets/lip_reading) | 19.513 | 0.688 | 0.508 | [download](https://drive.google.com/file/d/1L3MeKwACURdJ-ihxFml0kbK6B1ZyLk57/view)

<br>

## 1. Installation
```bash
conda create -y -n aligndit python=3.10 && conda activate aligndit

git clone https://github.com/kaistmm/AlignDiT.git && cd AlignDiT

pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121
pip install -e .
pip install -e .[eval]  # For evaluation
```

## 2. Data Preparation
We crop the mouth region from each video following [Auto-AVSR](https://github.com/mpc001/auto_avsr) and place the resulting videos in `data/LRS3_debug/autoavsr/video`. For both training and inference, we use these cropped videos.

#### Metadata
```bash
src/aligndit/run/misc/prepare_librispeech_notext.sh
src/aligndit/run/misc/prepare_lrs3.sh
```

#### Mel spectrogram
```bash
src/aligndit/run/misc/extract_mel.sh
```

#### HuBERT feature
```bash
src/aligndit/run/misc/extract_hubert.sh
```

#### AV-HuBERT video feature
This requires [Fairseq](https://github.com/facebookresearch/fairseq) and [AV-HuBERT](https://github.com/facebookresearch/av_hubert).
```bash
src/aligndit/run/misc/extract_avhubert_from_only_video.sh
```

## 3. Training
```bash
# 1. Pre-train on LibriSpeech for 500k updates
src/aligndit/run/train/pretrain.sh

# 2. Fine-tune on LRS3 for 400k updates
src/aligndit/run/train/finetune.sh
```

## 4. Inference
```bash
# ADR (automated dialogue replacement)
src/aligndit/run/eval/infer.sh

# VTS (video-to-speech synthesis)
src/aligndit/run/eval/infer_w_lipreader.sh
```

## 5. Evaluation
We follow [F5-TTS](https://github.com/SWivid/F5-TTS) for evaluation. Further details are avilable [here](https://github.com/SWivid/F5-TTS/tree/main/src/f5_tts/eval).
```bash
src/aligndit/run/eval/eval_lrs3_test.sh
```

<br>

## Acknowledgement
This repository is built using [F5-TTS](https://github.com/SWivid/F5-TTS), [AV-HuBERT](https://github.com/facebookresearch/av_hubert), [Fairseq](https://github.com/pytorch/fairseq), [CosyVoice](https://github.com/FunAudioLLM/CosyVoice), [HiFi-GAN](https://github.com/jik876/hifi-gan), [V2SFlow](https://github.com/kaistmm/V2SFlow). We appreciate the open source of the projects.

## Citation
If our work is useful for you, please cite the following paper:
```bibtex
@inproceedings{choi2025aligndit,
  title={AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation},
  author={Choi, Jeongsoo and Kim, Ji-Hoon and Sung-Bin, Kim and Oh, Tae-Hyun and Chung, Joon Son},
  booktitle={Proceedings of the 33rd ACM International Conference on Multimedia},
  year={2025}
}
```

## License
This project is released under the MIT License. Please note that the use of [AV-HuBERT](https://github.com/facebookresearch/av_hubert) models is subject to their original license terms.
